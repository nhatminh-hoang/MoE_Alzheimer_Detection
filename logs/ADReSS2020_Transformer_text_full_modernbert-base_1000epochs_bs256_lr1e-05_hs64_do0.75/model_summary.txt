==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TransformerModel                         [108, 1]                  --
├─Linear: 1-1                            [108, 300, 64]            49,216
├─ModuleList: 1-2                        --                        --
│    └─TransformerBlock: 2-1             [108, 300, 64]            --
│    │    └─MultiHeadAttention: 3-1      [108, 300, 64]            16,768
│    │    └─FFN: 3-2                     [108, 300, 64]            132,288
│    └─TransformerBlock: 2-2             [108, 300, 64]            --
│    │    └─MultiHeadAttention: 3-3      [108, 300, 64]            16,768
│    │    └─FFN: 3-4                     [108, 300, 64]            132,288
│    └─TransformerBlock: 2-3             [108, 300, 64]            --
│    │    └─MultiHeadAttention: 3-5      [108, 300, 64]            16,768
│    │    └─FFN: 3-6                     [108, 300, 64]            132,288
│    └─TransformerBlock: 2-4             [108, 300, 64]            --
│    │    └─MultiHeadAttention: 3-7      [108, 300, 64]            16,768
│    │    └─FFN: 3-8                     [108, 300, 64]            132,288
├─Linear: 1-3                            [108, 1]                  19,201
==========================================================================================
Total params: 664,641
Trainable params: 664,641
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 71.78
==========================================================================================
Input size (MB): 99.53
Forward/backward pass size (MB): 1542.76
Params size (MB): 2.66
Estimated Total Size (MB): 1644.95
==========================================================================================